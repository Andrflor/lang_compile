mov_r64_imm64 :: proc(reg: Register64, imm: u64) -> [10]u8
mov_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
mov_m64_r64 :: proc(mem_addr: u64, src: Register64) -> [8]u8
movabs_r64_imm64 :: proc(reg: Register64, imm: u64) -> [10]u8
xchg_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
movsx_r64_r32 :: proc(dst: Register64, src: Register64) -> [3]u8
movzx_r64_r32 :: proc(dst: Register64, src: Register64) -> [3]u8
movbe_r64_m64 :: proc(dst: Register64, mem_addr: u64) -> [10]u8
movbe_m64_r64 :: proc(mem_addr: u64, src: Register64) -> [10]u8
bswap_r64 :: proc(reg: Register64) -> [3]u8
mov_cr_r64 :: proc(cr: u8, reg: Register64) -> [3]u8
mov_r64_cr :: proc(reg: Register64, cr: u8) -> [3]u8
mov_cr0_r64 :: proc(reg: Register64) -> [3]u8
mov_cr2_r64 :: proc(reg: Register64) -> [3]u8
mov_cr3_r64 :: proc(reg: Register64) -> [3]u8
mov_cr4_r64 :: proc(reg: Register64) -> [3]u8
mov_dr0_r64 :: proc(reg: Register64) -> [3]u8
mov_dr1_r64 :: proc(reg: Register64) -> [3]u8
mov_dr2_r64 :: proc(reg: Register64) -> [3]u8
mov_dr3_r64 :: proc(reg: Register64) -> [3]u8
mov_dr6_r64 :: proc(reg: Register64) -> [3]u8
mov_dr7_r64 :: proc(reg: Register64) -> [3]u8
lea_r64_m64 :: proc(dst: Register64, mem_addr: u64) -> [8]u8
lea_r64_m :: proc(dst: Register64, mem: u64) -> [8]u8
add_r64_imm32 :: proc(reg: Register64, imm: u32) -> [7]u8
add_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
sub_r64_imm32 :: proc(reg: Register64, imm: u32) -> [7]u8
sub_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
inc_r64 :: proc(reg: Register64) -> [3]u8
dec_r64 :: proc(reg: Register64) -> [3]u8
neg_r64 :: proc(reg: Register64) -> [3]u8
adc_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
sbb_r64_r64 :: proc(dst: Register64, src: Register64) -> [3]u8
xadd_r64_r64 :: proc(dst: Register64, src: Register64) -> [4]u8
mul_r64 :: proc(reg: Register64) -> [3]u8
imul_r64_r64 :: proc(dst: Register64, src: Register64) -> [4]u8
imul_r64_r64_imm32 :: proc(dst: Register64, src: Register64, imm: u32) -> [7]u8
imul_r64_imm32 :: proc(reg: Register64, imm: u32) -> [7]u8
div_r64 :: proc(reg: Register64) -> [3]u8
idiv_r64 :: proc(reg: Register64) -> [3]u8

// --- Basic Bitwise Operations ---
and_r64_r64 :: proc(dst: Register64, src: Register64)
or_r64_r64 :: proc(dst: Register64, src: Register64)
xor_r64_r64 :: proc(dst: Register64, src: Register64)
not_r64 :: proc(reg: Register64)

// --- Shifts and Rotates ---
shl_r64_imm8 :: proc(reg: Register64, imm: u8)
	assert(imm <= 63, "Shift amount must be 0-63")
}
shr_r64_imm8 :: proc(reg: Register64, imm: u8)
	assert(imm <= 63, "Shift amount must be 0-63")
}
rol_r64_imm8 :: proc(reg: Register64, imm: u8)
	assert(imm <= 63, "Rotate amount must be 0-63")
}
ror_r64_imm8 :: proc(reg: Register64, imm: u8)
	assert(imm <= 63, "Rotate amount must be 0-63")
}
shld_r64_r64_imm8 :: proc(dst: Register64, src: Register64, imm: u8)
shrd_r64_r64_imm8 :: proc(dst: Register64, src: Register64, imm: u8)

// --- Bit Manipulation ---
bt_r64_r64 :: proc(reg: Register64, bit_index: Register64)
bts_r64_r64 :: proc(reg: Register64, bit_index: Register64)
btr_r64_r64 :: proc(reg: Register64, bit_index: Register64)
btc_r64_r64 :: proc(reg: Register64, bit_index: Register64)
bsf_r64_r64 :: proc(dst: Register64, src: Register64)
bsr_r64_r64 :: proc(dst: Register64, src: Register64)
popcnt_r64_r64 :: proc(dst: Register64, src: Register64)
lzcnt_r64_r64 :: proc(dst: Register64, src: Register64)
tzcnt_r64_r64 :: proc(dst: Register64, src: Register64)
pext_r64_r64_r64 :: proc(dst: Register64, src1: Register64, src2: Register64)
pdep_r64_r64_r64 :: proc(dst: Register64, src1: Register64, src2: Register64)

// ===== COMPARISON INSTRUCTIONS =====
cmp_r64_r64 :: proc(reg1: Register64, reg2: Register64)
cmp_r64_imm32 :: proc(reg: Register64, imm: u32)
test_r64_r64 :: proc(reg1: Register64, reg2: Register64)
test_r64_imm32 :: proc(reg: Register64, imm: u32)

// --- Conditional Moves ---
cmove_r64_r64 :: proc(dst: Register64, src: Register64)
cmovne_r64_r64 :: proc(dst: Register64, src: Register64)
cmova_r64_r64 :: proc(dst: Register64, src: Register64)
cmovae_r64_r64 :: proc(dst: Register64, src: Register64)
cmovb_r64_r64 :: proc(dst: Register64, src: Register64)
cmovbe_r64_r64 :: proc(dst: Register64, src: Register64)

// ===== CONTROL FLOW INSTRUCTIONS =====
// --- Unconditional Jumps & Calls ---
jmp_rel32 :: proc(offset: i32)
jmp_r64 :: proc(reg: Register64)
jmp_m64 :: proc(mem_addr: u64)
call_rel32 :: proc(offset: i32)
call_r64 :: proc(reg: Register64)
call_m64 :: proc(mem_addr: u64)
ret :: proc()

// --- Conditional Jumps ---
je_rel32 :: proc(offset: i32)
jne_rel32 :: proc(offset: i32)
jg_rel32 :: proc(offset: i32)
jl_rel32 :: proc(offset: i32)
jge_rel32 :: proc(offset: i32)
jle_rel32 :: proc(offset: i32)
ja_rel32 :: proc(offset: i32)
jae_rel32 :: proc(offset: i32)
jb_rel32 :: proc(offset: i32)
jbe_rel32 :: proc(offset: i32)

// --- Loop Control ---
loop_rel8 :: proc(offset: i8)
loope_rel8 :: proc(offset: i8)
loopne_rel8 :: proc(offset: i8)
jecxz_rel8 :: proc(offset: i8)

// --- Miscellaneous Control Flow ---
setcc_r8 :: proc(dst: u8)
endbr64 :: proc()

// ===== STACK MANIPULATION =====
push_r64 :: proc(reg: Register64)
pop_r64 :: proc(reg: Register64)
pushfq :: proc()
popfq :: proc()

// ===== SIMD INSTRUCTIONS =====
// --- SSE/AVX Data Movement ---
movd_xmm_r64 :: proc(xmm: XMMRegister, reg: Register64)
movd_r64_xmm :: proc(reg: Register64, xmm: XMMRegister)
movups_xmm_m128 :: proc(dst: XMMRegister, mem: u64)
movdqu_xmm_m128 :: proc(dst: XMMRegister, mem: u64)

// --- SSE/AVX Arithmetic ---
addps_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
mulps_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
divps_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
sqrtps_xmm :: proc(xmm: XMMRegister)

// --- SSE/AVX Comparison ---
cmpps_xmm_xmm_imm8 :: proc(dst: XMMRegister, src: XMMRegister, imm: u8)
cmpeqps_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
cmpneqps_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)

// --- AVX FMA (Fused Multiply-Add) ---
vfmadd132ps_xmm_xmm_xmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vfmadd213ps_xmm_xmm_xmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vfmadd231ps_xmm_xmm_xmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)

// --- AVX Advanced Operations ---
vaddps_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vmulps_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vdivps_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vblendps_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister, imm: u8)
vpand_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vpor_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vpxor_ymm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vpternlogd_ymm_ymm_ymm_imm8 :: proc(
	dst: XMMRegister,
	src1: XMMRegister,
	src2: XMMRegister,
	imm: u8,
)
vextracti128_ymm_ymm_imm8 :: proc(dst: XMMRegister, src: XMMRegister, imm: u8)

// --- SIMD Integer Operations ---
pavgb_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
pavgb_ymm_ymm :: proc(dst: XMMRegister, src: XMMRegister)
pmaddwd_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)
pmulhuw_xmm_xmm :: proc(dst: XMMRegister, src: XMMRegister)

// --- AVX-512 Operations ---
vaddpd_zmm_zmm_zmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vsubpd_zmm_zmm_zmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vmulpd_zmm_zmm_zmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vdivpd_zmm_zmm_zmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vgatherdps_xmm :: proc(dst: XMMRegister)
vscatterdps_xmm :: proc(dst: XMMRegister)
kmovq_k_k :: proc(dst: u8, src: u8)
vpxordq_zmm_zmm_zmm :: proc(dst: XMMRegister, src1: XMMRegister, src2: XMMRegister)
vpscatterdd_ymm_m :: proc(src: XMMRegister, mem: u64)
vpscatterdq_ymm_m :: proc(src: XMMRegister, mem: u64)
vpscatterqd_ymm_m :: proc(src: XMMRegister, mem: u64)
vpcompressd_ymm_ymm :: proc(dst: XMMRegister, src: XMMRegister)
vpcompressq_ymm_ymm :: proc(dst: XMMRegister, src: XMMRegister)

// ===== CRYPTOGRAPHY INSTRUCTIONS =====
aesenc :: proc(dst: XMMRegister, src: XMMRegister)
aesdec :: proc(dst: XMMRegister, src: XMMRegister)
pclmulqdq :: proc(dst: XMMRegister, src: XMMRegister, imm: u8)
crc32_r64_r64 :: proc(dst: Register64, src: Register64)

// ===== X87 FLOATING POINT INSTRUCTIONS =====
fadd_st0_st :: proc(src: x87Register)
fsub_st0_st :: proc(src: x87Register)
fmul_st0_st :: proc(src: x87Register)
fdiv_st0_st :: proc(src: x87Register)
fcom_st :: proc(src: x87Register)
fcomp_st :: proc(src: x87Register)
fucom_st :: proc(src: x87Register)
fucomp_st :: proc(src: x87Register)
fldcw :: proc(mem: u64)
fstcw :: proc(mem: u64)
fnstcw :: proc(mem: u64)
fninit :: proc()
fwait :: proc()

// ===== MEMORY AND STRING OPERATIONS =====
movs_m64_m64 :: proc()
stos_m64 :: proc()
cmps_m64_m64 :: proc()
rep_movs :: proc()
rep_stos :: proc()
rep_cmps :: proc()

// ===== SYSTEM INSTRUCTIONS =====
// --- Interrupts and System Calls ---
syscall :: proc()
sysret :: proc()
int_imm8 :: proc(imm: u8)
int3 :: proc()
into :: proc()
iret :: proc()

// --- Processor Control ---
cpuid :: proc()
rdtsc :: proc()
rdtscp :: proc()
rdmsr :: proc()
wrmsr :: proc()
rdpmc :: proc()
hlt :: proc()

// --- Process & Memory Management ---
swapgs :: proc()
wrpkru :: proc()
rdpkru :: proc()
clac :: proc()
stac :: proc()
ud2 :: proc()

// --- Virtualization Instructions ---
vmcall :: proc()
vmlaunch :: proc()
vmresume :: proc()
vmxoff :: proc()

// ===== ATOMIC OPERATIONS =====
lock_xadd_r64_r64 :: proc(dst: Register64, src: Register64)
lock_cmpxchg_r64_r64 :: proc(dst: Register64, src: Register64)
lock_inc_r64 :: proc(reg: Register64)
lock_dec_r64 :: proc(reg: Register64)
lock_xchg_r64_r64 :: proc(dst: Register64, src: Register64)
atomic_load_r64 :: proc(dst: Register64, src: u64)
atomic_store_r64 :: proc(dst: u64, src: Register64)

// ===== TRANSACTIONAL MEMORY INSTRUCTIONS =====
xbegin :: proc(offset: i32)
xend :: proc()
xabort :: proc(imm: u8)
xtest :: proc()

// ===== RANDOM NUMBER GENERATION =====
rdrand_r64 :: proc(reg: Register64)
rdseed_r64 :: proc(reg: Register64)

// ===== PREFETCH INSTRUCTIONS =====
prefetcht0 :: proc(mem: u64)
prefetcht1 :: proc(mem: u64)
prefetcht2 :: proc(mem: u64)
prefetchnta :: proc(mem: u64)

// ===== MEMORY MANAGEMENT AND OPTIMIZATION =====
clflush_m64 :: proc(mem: u64)
clflushopt_m64 :: proc(mem: u64)
clwb_m64 :: proc(mem: u64)
bound_r64_m128 :: proc(reg: Register64, mem: u64)

// ===== POWER MANAGEMENT =====
monitor_r64_r64_r64 :: proc(reg1: Register64, reg2: Register64, reg3: Register64)
mwait_r64_r64 :: proc(reg1: Register64, reg2: Register64)

// ===== MEMORY FENCES =====
mfence :: proc()
lfence :: proc()
sfence :: proc()

// ===== PERFORMANCE MONITORING =====
perfmon_instructions :: proc()

// Helper: automated test equality function.
test_equal :: proc(test_name: string, expected: []u8, actual: []u8)
	if len(expected) != len(actual)
		fmt.printf("FAIL: %s: expected length %d, got %d\n", test_name, len(expected), len(actual))
		return
	}
	for v, i in expected
		if v != actual[i]
			fmt.printf(
				"FAIL: %s: at index %d, expected 0x%X, got 0x%X\n",
				test_name,
				i,
				v,
				actual[i],
			)
			return
		}
	}
	fmt.printf("PASS: %s\n", test_name)
}

main :: proc()
	t1: [10]u8 = mov_r64_imm64(Register64.RAX, 0x123456789ABCDEF0)
	expected1: []u8 = asm_to_bytes("mov rax, 0x123456789ABCDEF0")
	test_equal("mov_r64_imm64", expected1, t1[:])

	// mov_r64_r64: mov rbx, rax
	t2: [3]u8 = mov_r64_r64(Register64.RBX, Register64.RAX)
	expected2: []u8 = asm_to_bytes("mov rbx, rax")
	fmt.println(expected2)
	test_equal("mov_r64_r64", expected2, t2[:])

	// mov_r64_m64: mov rcx, [0x00400000]
	t3: [8]u8 = mov_r64_m64(Register64.RCX, 0x00400000)
	expected3: [8]u8 = [8]u8{0x48, 0x8B, 0x0C, 0x25, 0x00, 0x00, 0x40, 0x00}
	test_equal("mov_r64_m64", expected3[:], t3[:])

	// mov_m64_r64: mov [0x00400000], rdx
	t4: [8]u8 = mov_m64_r64(0x00400000, Register64.RDX)
	expected4: [8]u8 = [8]u8{0x48, 0x89, 0x14, 0x25, 0x00, 0x00, 0x40, 0x00}
	test_equal("mov_m64_r64", expected4[:], t4[:])

	// movabs_r64_imm64: movabs rsi, 0xDEADBEEFCAFEBABE
	t5: [10]u8 = movabs_r64_imm64(Register64.RSI, 0xDEADBEEFCAFEBABE)
	expected5: [10]u8 = [10]u8{0x48, 0xBE, 0xBE, 0xBA, 0xFE, 0xCA, 0xEF, 0xBE, 0xAD, 0xDE}
	test_equal("movabs_r64_imm64", expected5[:], t5[:])

	// xchg_r64_r64: xchg rdi, rbx
	t6: [3]u8 = xchg_r64_r64(Register64.RDI, Register64.RBX)
	expected6: [3]u8 = [3]u8{0x48, 0x87, 0xDF}
	test_equal("xchg_r64_r64", expected6[:], t6[:])

	// movsx_r64_r32: movsx rax, ecx
	t7: [3]u8 = movsx_r64_r32(Register64.RAX, Register64.RCX)
	expected7: [3]u8 = [3]u8{0x48, 0x63, 0xC1}
	test_equal("movsx_r64_r32", expected7[:], t7[:])

	// movzx_r64_r32: movzx rdx, ebx
	t8: [3]u8 = movzx_r64_r32(Register64.RDX, Register64.RBX)
	expected8: [3]u8 = [3]u8{0x40, 0x89, 0xDA}
	test_equal("movzx_r64_r32", expected8[:], t8[:])

	// movbe_r64_m64: movbe rsi, [0x00400000]
	t9: [10]u8 = movbe_r64_m64(Register64.RSI, 0x00400000)
	expected9: [10]u8 = [10]u8{0x48, 0x0F, 0x38, 0xF0, 0x34, 0x25, 0x00, 0x00, 0x40, 0x00}
	test_equal("movbe_r64_m64", expected9[:], t9[:])

	// movbe_m64_r64: movbe [0x00400000], rdi
	t10: [10]u8 = movbe_m64_r64(0x00400000, Register64.RDI)
	expected10: [10]u8 = [10]u8{0x48, 0x0F, 0x38, 0xF1, 0x3C, 0x25, 0x00, 0x00, 0x40, 0x00}
	test_equal("movbe_m64_r64", expected10[:], t10[:])

	// bswap_r64: bswap rcx
	t11: [3]u8 = bswap_r64(Register64.RCX)
	expected11: [3]u8 = [3]u8{0x48, 0x0F, 0xC9}
	test_equal("bswap_r64", expected11[:], t11[:])

	// mov_cr_r64: mov cr3, rbx
	t12: [3]u8 = mov_cr_r64(3, Register64.RBX)
	expected12: [3]u8 = [3]u8{0x0F, 0x22, 0xDB}
	test_equal("mov_cr_r64", expected12[:], t12[:])

	// mov_r64_cr: mov rax, cr0
	t13: [3]u8 = mov_r64_cr(Register64.RAX, 0)
	expected13: [3]u8 = [3]u8{0x0F, 0x20, 0xC0}
	test_equal("mov_r64_cr", expected13[:], t13[:])

	// mov_cr0_r64: mov cr0, rdx
	t14: [3]u8 = mov_cr0_r64(Register64.RDX)
	expected14: [3]u8 = [3]u8{0x0F, 0x22, 0xC2}
	test_equal("mov_cr0_r64", expected14[:], t14[:])

	// mov_dr0_r64: mov dr0, rbx
	t15: [3]u8 = mov_dr0_r64(Register64.RBX)
	expected15: [3]u8 = [3]u8{0x0F, 0x23, 0xC3}
	test_equal("mov_dr0_r64", expected15[:], t15[:])

	// mov_dr1_r64: mov dr1, rcx
	t16: [3]u8 = mov_dr1_r64(Register64.RCX)
	expected16: [3]u8 = [3]u8{0x0F, 0x23, 0xC9}
	test_equal("mov_dr1_r64", expected16[:], t16[:])

	// mov_dr2_r64: mov dr2, rdx
	t17: [3]u8 = mov_dr2_r64(Register64.RDX)
	expected17: [3]u8 = [3]u8{0x0F, 0x23, 0xD2}
	test_equal("mov_dr2_r64", expected17[:], t17[:])

	// mov_dr3_r64: mov dr3, rdi
	t18: [3]u8 = mov_dr3_r64(Register64.RDI)
	expected18: [3]u8 = [3]u8{0x0F, 0x23, 0xDF}
	test_equal("mov_dr3_r64", expected18[:], t18[:])

	// mov_dr6_r64: mov dr6, rax
	t19: [3]u8 = mov_dr6_r64(Register64.RAX)
	expected19: [3]u8 = [3]u8{0x0F, 0x23, 0xF0}
	test_equal("mov_dr6_r64", expected19[:], t19[:])

	// mov_dr7_r64: mov dr7, rbx
	t20: [3]u8 = mov_dr7_r64(Register64.RBX)
	expected20: [3]u8 = [3]u8{0x0F, 0x23, 0xFB}
	test_equal("mov_dr7_r64", expected20[:], t20[:])

	// lea_r64_m64: lea rsi, [0x00400000]
	t21: [8]u8 = lea_r64_m64(Register64.RSI, 0x00400000)
	expected21: [8]u8 = [8]u8{0x48, 0x8D, 0x34, 0x25, 0x00, 0x00, 0x40, 0x00}
	test_equal("lea_r64_m64", expected21[:], t21[:])

	// add_r64_imm32: add rbx, 0x12345678
	t22: [7]u8 = add_r64_imm32(Register64.RBX, 0x12345678)
	expected22: [7]u8 = [7]u8{0x48, 0x81, 0xC3, 0x78, 0x56, 0x34, 0x12}
	test_equal("add_r64_imm32", expected22[:], t22[:])

	// add_r64_r64: add rax, rcx
	t23: [3]u8 = add_r64_r64(Register64.RAX, Register64.RCX)
	expected23: [3]u8 = [3]u8{0x48, 0x01, 0xC8}
	test_equal("add_r64_r64", expected23[:], t23[:])

	// sub_r64_imm32: sub rdx, 0x87654321
	t24: [7]u8 = sub_r64_imm32(Register64.RDX, 0x87654321)
	expected24: [7]u8 = [7]u8{0x48, 0x81, 0xEA, 0x21, 0x43, 0x65, 0x87}
	test_equal("sub_r64_imm32", expected24[:], t24[:])

	// sub_r64_r64: sub rbx, rsi
	t25: [3]u8 = sub_r64_r64(Register64.RBX, Register64.RSI)
	expected25: [3]u8 = [3]u8{0x48, 0x29, 0xF3}
	test_equal("sub_r64_r64", expected25[:], t25[:])

	// inc_r64: inc rax
	t26: [3]u8 = inc_r64(Register64.RAX)
	expected26: [3]u8 = [3]u8{0x48, 0xFF, 0xC0}
	test_equal("inc_r64", expected26[:], t26[:])

	// dec_r64: dec rcx
	t27: [3]u8 = dec_r64(Register64.RCX)
	expected27: [3]u8 = [3]u8{0x48, 0xFF, 0xC9}
	test_equal("dec_r64", expected27[:], t27[:])

	// neg_r64: neg rdx
	t28: [3]u8 = neg_r64(Register64.RDX)
	expected28: [3]u8 = [3]u8{0x48, 0xF7, 0xDA}
	test_equal("neg_r64", expected28[:], t28[:])

	// adc_r64_r64: adc rbx, rsi
	t29: [3]u8 = adc_r64_r64(Register64.RBX, Register64.RSI)
	expected29: [3]u8 = [3]u8{0x48, 0x11, 0xF3}
	test_equal("adc_r64_r64", expected29[:], t29[:])

	// sbb_r64_r64: sbb rdi, rbx
	t30: [3]u8 = sbb_r64_r64(Register64.RDI, Register64.RBX)
	expected30: [3]u8 = [3]u8{0x48, 0x19, 0xDF}
	test_equal("sbb_r64_r64", expected30[:], t30[:])

	// xadd_r64_r64: xadd rdx, rbx
	t31: [4]u8 = xadd_r64_r64(Register64.RDX, Register64.RBX)
	expected31: [4]u8 = [4]u8{0x48, 0x0F, 0xC1, 0xDA}
	test_equal("xadd_r64_r64", expected31[:], t31[:])

	// mul_r64: mul rsi
	t32: [3]u8 = mul_r64(Register64.RSI)
	expected32: [3]u8 = [3]u8{0x48, 0xF7, 0xE6}
	test_equal("mul_r64", expected32[:], t32[:])

	// imul_r64_r64: imul rax, rdi
	t33: [4]u8 = imul_r64_r64(Register64.RAX, Register64.RDI)
	expected33: [4]u8 = [4]u8{0x48, 0x0F, 0xAF, 0xC7}
	test_equal("imul_r64_r64", expected33[:], t33[:])

	// imul_r64_r64_imm32: imul rbx, rcx, 0x11223344
	t34: [7]u8 = imul_r64_r64_imm32(Register64.RBX, Register64.RCX, 0x11223344)
	expected34: [7]u8 = [7]u8{0x48, 0x69, 0xD9, 0x44, 0x33, 0x22, 0x11}
	test_equal("imul_r64_r64_imm32", expected34[:], t34[:])

	// imul_r64_imm32: imul rdx, 0x55667788
	t35: [7]u8 = imul_r64_imm32(Register64.RDX, 0x55667788)
	expected35: []u8 = []u8{0x48, 0x69, 0xD2, 0x88, 0x77, 0x66, 0x55}
	test_equal("imul_r64_imm32", expected35, t35[:])

	// div_r64: div rsi
	t36: [3]u8 = div_r64(Register64.RSI)
	expected36: [3]u8 = [3]u8{0x48, 0xF7, 0xF6}
	test_equal("div_r64", expected36[:], t36[:])

	// idiv_r64: idiv rdi
	t37: [3]u8 = idiv_r64(Register64.RDI)
	expected37: [3]u8 = [3]u8{0x48, 0xF7, 0xFF}
	test_equal("idiv_r64", expected37[:], t37[:])
}
